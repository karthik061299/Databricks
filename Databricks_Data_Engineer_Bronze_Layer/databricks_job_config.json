{
  "job_configuration": {
    "name": "bronze_layer_inventory_ingestion_enhanced",
    "description": "Enhanced Bronze Layer Data Ingestion Pipeline for Inventory Management System",
    "tags": {
      "environment": "production",
      "team": "data-engineering",
      "project": "inventory-management",
      "version": "2.0"
    },
    "tasks": [
      {
        "task_key": "bronze_ingestion_enhanced",
        "description": "Enhanced bronze layer ingestion with quality checks and retry logic",
        "notebook_task": {
          "notebook_path": "/Repos/bronze_layer/bronze_layer_ingestion_notebook",
          "base_parameters": {
            "pipeline_version": "2.0",
            "source_system": "PostgreSQL",
            "target_schema": "workspace.inventory_bronze",
            "max_retries": "3",
            "enable_quality_checks": "true",
            "enable_parallel_processing": "true"
          }
        },
        "existing_cluster_id": "0804-223344-batchcluster",
        "timeout_seconds": 3600,
        "max_retries": 2,
        "min_retry_interval_millis": 10000,
        "retry_on_timeout": true,
        "libraries": [
          {
            "maven": {
              "coordinates": "org.postgresql:postgresql:42.5.0"
            }
          }
        ]
      }
    ],
    "schedule": {
      "quartz_cron_expression": "0 0 20 * * ?",
      "timezone_id": "Asia/Kolkata",
      "pause_status": "UNPAUSED"
    },
    "email_notifications": {
      "on_start": [],
      "on_success": [],
      "on_failure": ["data-engineering-team@company.com"],
      "no_alert_for_skipped_runs": false
    },
    "webhook_notifications": {
      "on_start": [],
      "on_success": [],
      "on_failure": []
    },
    "timeout_seconds": 7200,
    "max_concurrent_runs": 1,
    "format": "MULTI_TASK",
    "access_control_list": [
      {
        "user_name": "data-engineer@company.com",
        "permission_level": "IS_OWNER"
      },
      {
        "group_name": "data-engineering-team",
        "permission_level": "CAN_MANAGE_RUN"
      }
    ]
  },
  "deployment_instructions": {
    "prerequisites": [
      "Databricks workspace with Unity Catalog enabled",
      "Azure Key Vault configured with database credentials",
      "PostgreSQL JDBC driver available in cluster",
      "Target schema 'workspace.inventory_bronze' created",
      "Appropriate permissions for Unity Catalog access"
    ],
    "setup_steps": [
      "1. Upload the bronze_layer_ingestion_notebook.py to Databricks workspace",
      "2. Create or verify the cluster with ID: 0804-223344-batchcluster",
      "3. Install PostgreSQL JDBC driver on the cluster",
      "4. Configure Azure Key Vault secrets access",
      "5. Create the job using this configuration via Databricks UI or API",
      "6. Test the job with a manual run before enabling schedule"
    ],
    "validation_steps": [
      "1. Verify source database connectivity",
      "2. Check target schema permissions",
      "3. Validate audit table creation",
      "4. Run data quality checks",
      "5. Monitor job execution logs",
      "6. Verify data in bronze layer tables"
    ]
  },
  "monitoring_and_alerting": {
    "key_metrics": [
      "Job execution duration",
      "Number of records processed per table",
      "Data quality scores",
      "Error rates and retry counts",
      "Resource utilization"
    ],
    "alert_conditions": [
      "Job failure after all retries",
      "Data quality score below 0.8",
      "Processing time exceeds 2 hours",
      "Zero records processed for any table",
      "Connection failures to source database"
    ],
    "dashboard_metrics": [
      "Daily ingestion volume trends",
      "Processing time by table",
      "Data quality trends over time",
      "Error categorization and frequency",
      "Cost per execution analysis"
    ]
  },
  "performance_tuning": {
    "cluster_configuration": {
      "recommended_node_type": "Standard_DS3_v2",
      "min_workers": 2,
      "max_workers": 8,
      "auto_termination_minutes": 30,
      "spark_version": "11.3.x-scala2.12",
      "spark_conf": {
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true",
        "spark.databricks.delta.optimizeWrite.enabled": "true",
        "spark.databricks.delta.autoCompact.enabled": "true"
      }
    },
    "optimization_recommendations": [
      "Enable auto-scaling for variable workloads",
      "Use spot instances for cost optimization",
      "Implement table partitioning for large datasets",
      "Schedule OPTIMIZE operations for Delta tables",
      "Monitor and tune JDBC fetch sizes"
    ]
  },
  "security_configuration": {
    "access_controls": {
      "unity_catalog": "Required for table access",
      "azure_key_vault": "Required for credential management",
      "cluster_policies": "Apply data engineering cluster policy"
    },
    "data_governance": {
      "pii_handling": "Identified PII columns in customers and suppliers tables",
      "audit_logging": "Comprehensive audit trail in bz_audit_log table",
      "data_lineage": "Tracked through metadata columns and audit logs",
      "retention_policy": "Follow company data retention guidelines"
    }
  },
  "cost_analysis": {
    "estimated_costs": {
      "compute_cost_per_run": "$2.50 - $5.00 (depending on data volume)",
      "storage_cost_monthly": "$0.50 - $2.00 per GB (Delta Lake storage)",
      "api_processing_cost": "$0.000625 per pipeline execution"
    },
    "cost_optimization_strategies": [
      "Use auto-terminating clusters",
      "Implement incremental loading for large tables",
      "Optimize Delta table storage with VACUUM",
      "Monitor and right-size cluster configurations",
      "Use spot instances where appropriate"
    ]
  },
  "troubleshooting_guide": {
    "common_issues": {
      "connection_failures": {
        "symptoms": "JDBC connection timeouts or authentication errors",
        "solutions": [
          "Verify Azure Key Vault access",
          "Check network connectivity to PostgreSQL",
          "Validate database credentials",
          "Review firewall and security group settings"
        ]
      },
      "performance_issues": {
        "symptoms": "Long execution times or resource exhaustion",
        "solutions": [
          "Increase cluster size or enable auto-scaling",
          "Optimize JDBC fetch sizes and partitioning",
          "Review and tune Spark configurations",
          "Implement incremental loading strategies"
        ]
      },
      "data_quality_issues": {
        "symptoms": "Low quality scores or data validation failures",
        "solutions": [
          "Review source data quality",
          "Adjust quality check thresholds",
          "Implement data cleansing rules",
          "Coordinate with source system owners"
        ]
      }
    }
  }
}